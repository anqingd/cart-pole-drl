{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-learning for Cart-Pole\n",
    "\n",
    "This notebook uses OpenAI Gym and Deep Q-learning to creating a playing agent for Cart-Pole. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies and create a Cart-Pole playing environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the OpenAI Gym environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a list of the possible actions for this game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two possible actions, moving the cart left and right--coded as 0 or 1 in the environment\n",
    "\n",
    "---\n",
    "\n",
    "Let's run a random simulation to see how the game it played"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.reset()\n",
    "rewards = []\n",
    "for move in range(100):\n",
    "    env.render()\n",
    "    state, reward, done, info = env.step(env.action_space.sample())\n",
    "    rewards.append(reward)\n",
    "    if done:\n",
    "        rewards = []\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object of the game is to move the cart left or right to keep the pole from falling. The longer the pole stays up, the more reward we receive. For this game, we get a reward of 1 for each step that the pole is still standing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Q-Network\n",
    "\n",
    "In reinforcement learning we usually keep a matrix of all state-action pairs and update the values to help the agent learn. For some games, such as cart-pole, the number of state-action paris is simply too large for this to be feasible. Even for a simple game like cart-pole, there are four real-valued numbers that make up each possible state--position and velocity of the cart, and position and velocity of the pole. This creates a nearly infinite number of states.\n",
    "\n",
    "In deep Q-learning, we use a neural network to approximate the Q-table. Our A-network takes a state as input and outputs q-values for each possible action. \n",
    "\n",
    "Our targets for training are $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$, thus we want to minimize $(\\hat{Q}(s,a) - Q(s,a))^2$. This can be thought of as a measurement of how much reward can be expected in the next time step if we take a given action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QNetwork():\n",
    "    def __init__(self, learning_rate=0.01, state_size=4, action_size=2, hidden_size=10, name='QNetwork'):\n",
    "        with tf.variable_scope(name):\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, state_size], name='inputs')\n",
    "            self.actions_ = tf.placeholder(tf.int32, [None], name='actions')\n",
    "            one_hot_actions = tf.one_hot(self.actions_, action_size)\n",
    "\n",
    "            # Target placeholder for training\n",
    "            self.targetQs_ = tf.placeholder(tf.float32, [None], name='target')\n",
    "            \n",
    "            # Hidden layers\n",
    "            self.fc1 = tf.contrib.layers.fully_connected(self.inputs_, hidden_size)\n",
    "            self.fc2 = tf.contrib.layers.fully_connected(self.fc1, hidden_size)\n",
    "            \n",
    "            # Output layer\n",
    "            self.output = tf.contrib.layers.fully_connected(self.fc2, action_size, activation_fn=None)\n",
    "            \n",
    "            # Trian on (targetQ - Q)^2\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, one_hot_actions), axis=1)\n",
    "            \n",
    "            self.loss = tf.reduce_mean(tf.square(self.targetQs_ - self.Q))\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### State Memory \n",
    "Reinforcement learning algorithms can have stability issues due to correlations between states. Thus, it's usually not a good idea to train in sequential states as the agent plays the game. Instead, we will let the agent play the game, store the experiences in memory, and then train the network on a random sample of past experiences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory():\n",
    "    def __init__(self, max_size=1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "        \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        index_list = np.random.choice(np.arange(len(self.buffer)), size=batch_size, replace=False)\n",
    "        return [self.buffer[index] for index in index_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration vs. Exploitation\n",
    "In order for the agent to learn, it needs to explore its environmnet by taking random actions. As the agent learns, we want to take advantage of its exploration early on and choose what it thinks is the best action (exploit). At each step in the game we will decide whether the agent will explore or exploit. At the start of the game exploration will be more likely, but as the game progresses we will push the agent to exploit more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Algorithm\n",
    "The network will be trained in *episodes*, which is the same as one simulation of the game. For Cart-Pole, the goal of an episode is to keep the pole upright for 195 frames. We start a new episode when meeting that goal or if the game ends because the pole tilts too far or the cart tries to move off the screen. This is how we'll train the agent.\n",
    "\n",
    "* Initialize the memory $D$\n",
    "* Initialize the action-value network $Q$ with random weights\n",
    "* **For** episode = 1, $M$ **do**\n",
    "  * **For** $t$, $T$ **do**\n",
    "     * With probability $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s,a)$\n",
    "     * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "     * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "     * Sample random mini-batch from $D$: $<s_j, a_j, r_j, s'_j>$\n",
    "     * Set $\\hat{Q}_j = r_j$ if the episode ends at $j+1$, otherwise set $\\hat{Q}_j = r_j + \\gamma \\max_{a'}{Q(s'_j, a')}$\n",
    "     * Make a gradient descent step with loss $(\\hat{Q}_j - Q(s_j, a_j))^2$\n",
    "  * **endfor**\n",
    "* **endfor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_episodes = 1000          # max number of episodes to learn from\n",
    "max_steps = 200                # max steps in an episode\n",
    "gamma = 0.99                   # future reward discount\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "hidden_size = 64               # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.0001         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 10000            # memory capacity\n",
    "batch_size = 20                # experience mini-batch size\n",
    "pretrain_length = batch_size   # number experiences to pretrain the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = QNetwork(name='main', hidden_size=hidden_size, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Memory\n",
    "Reset the simulation and populate the memory with a set of transitions to train on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the simulation\n",
    "env.reset()\n",
    "# Take one random step to get the pole and cart moving\n",
    "state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "memory = Memory(max_size=memory_size)\n",
    "\n",
    "for index in range(pretrain_length):\n",
    "    # Uncomment to watch simulation\n",
    "    # env.render()\n",
    "    \n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "    if done:\n",
    "        next_state = np.zeros(state.shape)\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        env.reset()\n",
    "        state, reward, done, _ = env.step(env.action_space.sample())\n",
    "    else:\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Total reward: 13.0 Training loss: 0.9675 Explore P: 0.9987\n",
      "Episode: 2 Total reward: 17.0 Training loss: 0.9660 Explore P: 0.9970\n",
      "Episode: 3 Total reward: 20.0 Training loss: 0.9708 Explore P: 0.9951\n",
      "Episode: 4 Total reward: 41.0 Training loss: 1.0424 Explore P: 0.9910\n",
      "Episode: 5 Total reward: 21.0 Training loss: 1.0693 Explore P: 0.9890\n",
      "Episode: 6 Total reward: 15.0 Training loss: 1.1583 Explore P: 0.9875\n",
      "Episode: 7 Total reward: 14.0 Training loss: 1.0468 Explore P: 0.9861\n",
      "Episode: 8 Total reward: 15.0 Training loss: 1.2851 Explore P: 0.9847\n",
      "Episode: 9 Total reward: 20.0 Training loss: 1.2862 Explore P: 0.9827\n",
      "Episode: 10 Total reward: 19.0 Training loss: 1.3787 Explore P: 0.9809\n",
      "Episode: 11 Total reward: 48.0 Training loss: 1.3711 Explore P: 0.9762\n",
      "Episode: 12 Total reward: 17.0 Training loss: 2.0318 Explore P: 0.9746\n",
      "Episode: 13 Total reward: 53.0 Training loss: 3.0323 Explore P: 0.9695\n",
      "Episode: 14 Total reward: 21.0 Training loss: 2.9219 Explore P: 0.9675\n",
      "Episode: 15 Total reward: 11.0 Training loss: 2.9869 Explore P: 0.9664\n",
      "Episode: 16 Total reward: 12.0 Training loss: 3.1774 Explore P: 0.9653\n",
      "Episode: 17 Total reward: 13.0 Training loss: 3.8460 Explore P: 0.9640\n",
      "Episode: 18 Total reward: 13.0 Training loss: 3.9694 Explore P: 0.9628\n",
      "Episode: 19 Total reward: 29.0 Training loss: 4.5169 Explore P: 0.9600\n",
      "Episode: 20 Total reward: 27.0 Training loss: 5.1302 Explore P: 0.9575\n",
      "Episode: 21 Total reward: 11.0 Training loss: 3.6754 Explore P: 0.9564\n",
      "Episode: 22 Total reward: 36.0 Training loss: 4.5412 Explore P: 0.9530\n",
      "Episode: 23 Total reward: 9.0 Training loss: 5.7142 Explore P: 0.9522\n",
      "Episode: 24 Total reward: 13.0 Training loss: 7.9717 Explore P: 0.9510\n",
      "Episode: 25 Total reward: 19.0 Training loss: 9.2023 Explore P: 0.9492\n",
      "Episode: 26 Total reward: 10.0 Training loss: 12.0205 Explore P: 0.9482\n",
      "Episode: 27 Total reward: 9.0 Training loss: 8.9966 Explore P: 0.9474\n",
      "Episode: 28 Total reward: 36.0 Training loss: 12.8518 Explore P: 0.9440\n",
      "Episode: 29 Total reward: 16.0 Training loss: 17.4889 Explore P: 0.9425\n",
      "Episode: 30 Total reward: 16.0 Training loss: 11.4908 Explore P: 0.9410\n",
      "Episode: 31 Total reward: 20.0 Training loss: 18.6050 Explore P: 0.9392\n",
      "Episode: 32 Total reward: 18.0 Training loss: 13.6258 Explore P: 0.9375\n",
      "Episode: 33 Total reward: 15.0 Training loss: 21.1191 Explore P: 0.9361\n",
      "Episode: 34 Total reward: 9.0 Training loss: 22.3009 Explore P: 0.9353\n",
      "Episode: 35 Total reward: 34.0 Training loss: 21.2487 Explore P: 0.9321\n",
      "Episode: 36 Total reward: 34.0 Training loss: 33.7217 Explore P: 0.9290\n",
      "Episode: 37 Total reward: 20.0 Training loss: 31.9916 Explore P: 0.9272\n",
      "Episode: 38 Total reward: 16.0 Training loss: 60.3995 Explore P: 0.9257\n",
      "Episode: 39 Total reward: 58.0 Training loss: 36.8517 Explore P: 0.9204\n",
      "Episode: 40 Total reward: 51.0 Training loss: 55.0327 Explore P: 0.9158\n",
      "Episode: 41 Total reward: 19.0 Training loss: 86.4563 Explore P: 0.9141\n",
      "Episode: 42 Total reward: 17.0 Training loss: 53.6680 Explore P: 0.9125\n",
      "Episode: 43 Total reward: 25.0 Training loss: 79.6052 Explore P: 0.9103\n",
      "Episode: 44 Total reward: 11.0 Training loss: 75.7284 Explore P: 0.9093\n",
      "Episode: 45 Total reward: 13.0 Training loss: 87.7316 Explore P: 0.9081\n",
      "Episode: 46 Total reward: 23.0 Training loss: 141.8828 Explore P: 0.9061\n",
      "Episode: 47 Total reward: 19.0 Training loss: 81.5258 Explore P: 0.9044\n",
      "Episode: 48 Total reward: 16.0 Training loss: 46.0072 Explore P: 0.9029\n",
      "Episode: 49 Total reward: 19.0 Training loss: 163.1316 Explore P: 0.9012\n",
      "Episode: 50 Total reward: 8.0 Training loss: 100.9320 Explore P: 0.9005\n",
      "Episode: 51 Total reward: 11.0 Training loss: 122.2351 Explore P: 0.8995\n",
      "Episode: 52 Total reward: 39.0 Training loss: 191.1396 Explore P: 0.8961\n",
      "Episode: 53 Total reward: 23.0 Training loss: 77.7909 Explore P: 0.8940\n",
      "Episode: 54 Total reward: 27.0 Training loss: 74.5769 Explore P: 0.8917\n",
      "Episode: 55 Total reward: 38.0 Training loss: 153.7278 Explore P: 0.8883\n",
      "Episode: 56 Total reward: 15.0 Training loss: 124.7152 Explore P: 0.8870\n",
      "Episode: 57 Total reward: 65.0 Training loss: 249.4648 Explore P: 0.8813\n",
      "Episode: 58 Total reward: 22.0 Training loss: 261.0082 Explore P: 0.8794\n",
      "Episode: 59 Total reward: 17.0 Training loss: 309.9832 Explore P: 0.8779\n",
      "Episode: 60 Total reward: 42.0 Training loss: 448.3322 Explore P: 0.8743\n",
      "Episode: 61 Total reward: 17.0 Training loss: 390.5174 Explore P: 0.8728\n",
      "Episode: 62 Total reward: 19.0 Training loss: 580.9443 Explore P: 0.8712\n",
      "Episode: 63 Total reward: 22.0 Training loss: 183.2191 Explore P: 0.8693\n",
      "Episode: 64 Total reward: 15.0 Training loss: 321.8303 Explore P: 0.8680\n",
      "Episode: 65 Total reward: 14.0 Training loss: 471.7879 Explore P: 0.8668\n",
      "Episode: 66 Total reward: 10.0 Training loss: 218.6796 Explore P: 0.8659\n",
      "Episode: 67 Total reward: 21.0 Training loss: 262.3029 Explore P: 0.8641\n",
      "Episode: 68 Total reward: 25.0 Training loss: 217.6621 Explore P: 0.8620\n",
      "Episode: 69 Total reward: 23.0 Training loss: 278.0170 Explore P: 0.8601\n",
      "Episode: 70 Total reward: 20.0 Training loss: 473.8884 Explore P: 0.8584\n",
      "Episode: 71 Total reward: 31.0 Training loss: 603.6589 Explore P: 0.8557\n",
      "Episode: 72 Total reward: 13.0 Training loss: 504.6867 Explore P: 0.8546\n",
      "Episode: 73 Total reward: 24.0 Training loss: 494.1103 Explore P: 0.8526\n",
      "Episode: 74 Total reward: 23.0 Training loss: 685.5739 Explore P: 0.8507\n",
      "Episode: 75 Total reward: 20.0 Training loss: 382.1326 Explore P: 0.8490\n",
      "Episode: 76 Total reward: 15.0 Training loss: 179.0306 Explore P: 0.8477\n",
      "Episode: 77 Total reward: 25.0 Training loss: 354.6897 Explore P: 0.8456\n",
      "Episode: 78 Total reward: 12.0 Training loss: 773.4979 Explore P: 0.8446\n",
      "Episode: 79 Total reward: 14.0 Training loss: 583.0443 Explore P: 0.8435\n",
      "Episode: 80 Total reward: 14.0 Training loss: 649.0861 Explore P: 0.8423\n",
      "Episode: 81 Total reward: 20.0 Training loss: 811.3602 Explore P: 0.8406\n",
      "Episode: 82 Total reward: 25.0 Training loss: 415.8882 Explore P: 0.8386\n",
      "Episode: 83 Total reward: 12.0 Training loss: 752.1967 Explore P: 0.8376\n",
      "Episode: 84 Total reward: 12.0 Training loss: 754.6063 Explore P: 0.8366\n",
      "Episode: 85 Total reward: 16.0 Training loss: 582.1087 Explore P: 0.8353\n",
      "Episode: 86 Total reward: 40.0 Training loss: 492.0359 Explore P: 0.8320\n",
      "Episode: 87 Total reward: 12.0 Training loss: 908.2064 Explore P: 0.8310\n",
      "Episode: 88 Total reward: 16.0 Training loss: 635.7586 Explore P: 0.8297\n",
      "Episode: 89 Total reward: 12.0 Training loss: 279.3171 Explore P: 0.8287\n",
      "Episode: 90 Total reward: 34.0 Training loss: 1278.0381 Explore P: 0.8259\n",
      "Episode: 91 Total reward: 17.0 Training loss: 1044.1578 Explore P: 0.8245\n",
      "Episode: 92 Total reward: 16.0 Training loss: 873.2852 Explore P: 0.8232\n",
      "Episode: 93 Total reward: 8.0 Training loss: 881.5002 Explore P: 0.8226\n",
      "Episode: 94 Total reward: 25.0 Training loss: 1285.0396 Explore P: 0.8205\n",
      "Episode: 95 Total reward: 9.0 Training loss: 517.6041 Explore P: 0.8198\n",
      "Episode: 96 Total reward: 12.0 Training loss: 1171.2557 Explore P: 0.8188\n",
      "Episode: 97 Total reward: 18.0 Training loss: 685.5541 Explore P: 0.8174\n",
      "Episode: 98 Total reward: 11.0 Training loss: 961.9435 Explore P: 0.8165\n",
      "Episode: 99 Total reward: 9.0 Training loss: 986.1334 Explore P: 0.8158\n",
      "Episode: 100 Total reward: 15.0 Training loss: 1462.5885 Explore P: 0.8146\n",
      "Episode: 101 Total reward: 19.0 Training loss: 1318.3237 Explore P: 0.8130\n",
      "Episode: 102 Total reward: 13.0 Training loss: 384.2445 Explore P: 0.8120\n",
      "Episode: 103 Total reward: 65.0 Training loss: 1004.3076 Explore P: 0.8068\n",
      "Episode: 104 Total reward: 14.0 Training loss: 186.9066 Explore P: 0.8057\n",
      "Episode: 105 Total reward: 15.0 Training loss: 1075.5386 Explore P: 0.8045\n",
      "Episode: 106 Total reward: 13.0 Training loss: 1528.9117 Explore P: 0.8035\n",
      "Episode: 107 Total reward: 17.0 Training loss: 1534.9709 Explore P: 0.8021\n",
      "Episode: 108 Total reward: 12.0 Training loss: 709.4245 Explore P: 0.8012\n",
      "Episode: 109 Total reward: 23.0 Training loss: 348.3036 Explore P: 0.7993\n",
      "Episode: 110 Total reward: 9.0 Training loss: 1204.2787 Explore P: 0.7986\n",
      "Episode: 111 Total reward: 10.0 Training loss: 1356.0199 Explore P: 0.7978\n",
      "Episode: 112 Total reward: 11.0 Training loss: 1366.9812 Explore P: 0.7970\n",
      "Episode: 113 Total reward: 35.0 Training loss: 982.0991 Explore P: 0.7942\n",
      "Episode: 114 Total reward: 12.0 Training loss: 172.6328 Explore P: 0.7933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 115 Total reward: 29.0 Training loss: 762.5773 Explore P: 0.7910\n",
      "Episode: 116 Total reward: 20.0 Training loss: 113.4879 Explore P: 0.7895\n",
      "Episode: 117 Total reward: 12.0 Training loss: 2938.4565 Explore P: 0.7885\n",
      "Episode: 118 Total reward: 23.0 Training loss: 380.3663 Explore P: 0.7867\n",
      "Episode: 119 Total reward: 42.0 Training loss: 1827.1836 Explore P: 0.7835\n",
      "Episode: 120 Total reward: 19.0 Training loss: 738.6549 Explore P: 0.7820\n",
      "Episode: 121 Total reward: 25.0 Training loss: 1556.6135 Explore P: 0.7801\n",
      "Episode: 122 Total reward: 39.0 Training loss: 210.2809 Explore P: 0.7771\n",
      "Episode: 123 Total reward: 32.0 Training loss: 2026.9443 Explore P: 0.7746\n",
      "Episode: 124 Total reward: 9.0 Training loss: 854.8688 Explore P: 0.7740\n",
      "Episode: 125 Total reward: 11.0 Training loss: 1472.1055 Explore P: 0.7731\n",
      "Episode: 126 Total reward: 14.0 Training loss: 917.5771 Explore P: 0.7720\n",
      "Episode: 127 Total reward: 10.0 Training loss: 1327.7900 Explore P: 0.7713\n",
      "Episode: 128 Total reward: 24.0 Training loss: 2582.4011 Explore P: 0.7695\n",
      "Episode: 129 Total reward: 11.0 Training loss: 525.8008 Explore P: 0.7686\n",
      "Episode: 130 Total reward: 78.0 Training loss: 668.4571 Explore P: 0.7627\n",
      "Episode: 131 Total reward: 15.0 Training loss: 2130.6829 Explore P: 0.7616\n",
      "Episode: 132 Total reward: 12.0 Training loss: 773.8400 Explore P: 0.7607\n",
      "Episode: 133 Total reward: 19.0 Training loss: 804.9584 Explore P: 0.7593\n",
      "Episode: 134 Total reward: 15.0 Training loss: 3078.8110 Explore P: 0.7582\n",
      "Episode: 135 Total reward: 21.0 Training loss: 1647.8850 Explore P: 0.7566\n",
      "Episode: 136 Total reward: 10.0 Training loss: 1951.7520 Explore P: 0.7558\n",
      "Episode: 137 Total reward: 15.0 Training loss: 1368.2686 Explore P: 0.7547\n",
      "Episode: 138 Total reward: 20.0 Training loss: 1215.7532 Explore P: 0.7532\n",
      "Episode: 139 Total reward: 15.0 Training loss: 274.2395 Explore P: 0.7521\n",
      "Episode: 140 Total reward: 20.0 Training loss: 230.4950 Explore P: 0.7506\n",
      "Episode: 141 Total reward: 24.0 Training loss: 1193.9421 Explore P: 0.7489\n",
      "Episode: 142 Total reward: 14.0 Training loss: 1358.2345 Explore P: 0.7478\n",
      "Episode: 143 Total reward: 17.0 Training loss: 297.5845 Explore P: 0.7466\n",
      "Episode: 144 Total reward: 28.0 Training loss: 119.5228 Explore P: 0.7445\n",
      "Episode: 145 Total reward: 14.0 Training loss: 916.2711 Explore P: 0.7435\n",
      "Episode: 146 Total reward: 11.0 Training loss: 87.7580 Explore P: 0.7427\n",
      "Episode: 147 Total reward: 11.0 Training loss: 1055.4994 Explore P: 0.7419\n",
      "Episode: 148 Total reward: 16.0 Training loss: 703.2289 Explore P: 0.7407\n",
      "Episode: 149 Total reward: 35.0 Training loss: 937.1545 Explore P: 0.7381\n",
      "Episode: 150 Total reward: 12.0 Training loss: 882.1309 Explore P: 0.7373\n",
      "Episode: 151 Total reward: 12.0 Training loss: 1731.0232 Explore P: 0.7364\n",
      "Episode: 152 Total reward: 14.0 Training loss: 885.3797 Explore P: 0.7354\n",
      "Episode: 153 Total reward: 13.0 Training loss: 1619.2483 Explore P: 0.7344\n",
      "Episode: 154 Total reward: 28.0 Training loss: 228.4287 Explore P: 0.7324\n",
      "Episode: 155 Total reward: 39.0 Training loss: 1609.9501 Explore P: 0.7296\n",
      "Episode: 156 Total reward: 24.0 Training loss: 325.7436 Explore P: 0.7279\n",
      "Episode: 157 Total reward: 24.0 Training loss: 1423.2931 Explore P: 0.7262\n",
      "Episode: 158 Total reward: 19.0 Training loss: 874.4207 Explore P: 0.7248\n",
      "Episode: 159 Total reward: 24.0 Training loss: 179.9210 Explore P: 0.7231\n",
      "Episode: 160 Total reward: 13.0 Training loss: 831.6635 Explore P: 0.7222\n",
      "Episode: 161 Total reward: 12.0 Training loss: 1610.1190 Explore P: 0.7213\n",
      "Episode: 162 Total reward: 12.0 Training loss: 1156.4287 Explore P: 0.7205\n",
      "Episode: 163 Total reward: 18.0 Training loss: 115.9609 Explore P: 0.7192\n",
      "Episode: 164 Total reward: 16.0 Training loss: 838.3260 Explore P: 0.7180\n",
      "Episode: 165 Total reward: 10.0 Training loss: 932.1642 Explore P: 0.7173\n",
      "Episode: 166 Total reward: 29.0 Training loss: 1436.9056 Explore P: 0.7153\n",
      "Episode: 167 Total reward: 42.0 Training loss: 1121.8534 Explore P: 0.7123\n",
      "Episode: 168 Total reward: 25.0 Training loss: 1050.7109 Explore P: 0.7106\n",
      "Episode: 169 Total reward: 8.0 Training loss: 2263.0728 Explore P: 0.7100\n",
      "Episode: 170 Total reward: 17.0 Training loss: 754.1913 Explore P: 0.7088\n",
      "Episode: 171 Total reward: 15.0 Training loss: 242.5853 Explore P: 0.7078\n",
      "Episode: 172 Total reward: 10.0 Training loss: 14.3824 Explore P: 0.7071\n",
      "Episode: 173 Total reward: 15.0 Training loss: 2717.3523 Explore P: 0.7060\n",
      "Episode: 174 Total reward: 10.0 Training loss: 2166.8796 Explore P: 0.7053\n",
      "Episode: 175 Total reward: 14.0 Training loss: 1707.1052 Explore P: 0.7044\n",
      "Episode: 176 Total reward: 20.0 Training loss: 1428.0033 Explore P: 0.7030\n",
      "Episode: 177 Total reward: 9.0 Training loss: 877.4241 Explore P: 0.7024\n",
      "Episode: 178 Total reward: 28.0 Training loss: 1597.2201 Explore P: 0.7004\n",
      "Episode: 179 Total reward: 12.0 Training loss: 2841.0149 Explore P: 0.6996\n",
      "Episode: 180 Total reward: 8.0 Training loss: 1877.5339 Explore P: 0.6990\n",
      "Episode: 181 Total reward: 13.0 Training loss: 419.7427 Explore P: 0.6981\n",
      "Episode: 182 Total reward: 18.0 Training loss: 3001.1274 Explore P: 0.6969\n",
      "Episode: 183 Total reward: 18.0 Training loss: 1730.9014 Explore P: 0.6957\n",
      "Episode: 184 Total reward: 13.0 Training loss: 2910.7253 Explore P: 0.6948\n",
      "Episode: 185 Total reward: 15.0 Training loss: 344.0178 Explore P: 0.6938\n",
      "Episode: 186 Total reward: 13.0 Training loss: 1688.6475 Explore P: 0.6929\n",
      "Episode: 187 Total reward: 29.0 Training loss: 665.7811 Explore P: 0.6909\n",
      "Episode: 188 Total reward: 11.0 Training loss: 1632.6796 Explore P: 0.6901\n",
      "Episode: 189 Total reward: 13.0 Training loss: 968.2822 Explore P: 0.6893\n",
      "Episode: 190 Total reward: 12.0 Training loss: 1146.5726 Explore P: 0.6884\n",
      "Episode: 191 Total reward: 10.0 Training loss: 923.7888 Explore P: 0.6878\n",
      "Episode: 192 Total reward: 16.0 Training loss: 743.4669 Explore P: 0.6867\n",
      "Episode: 193 Total reward: 20.0 Training loss: 2888.7212 Explore P: 0.6853\n",
      "Episode: 194 Total reward: 10.0 Training loss: 959.7817 Explore P: 0.6847\n",
      "Episode: 195 Total reward: 16.0 Training loss: 347.6370 Explore P: 0.6836\n",
      "Episode: 196 Total reward: 11.0 Training loss: 3224.0610 Explore P: 0.6828\n",
      "Episode: 197 Total reward: 13.0 Training loss: 928.4479 Explore P: 0.6820\n",
      "Episode: 198 Total reward: 22.0 Training loss: 804.2597 Explore P: 0.6805\n",
      "Episode: 199 Total reward: 15.0 Training loss: 962.1266 Explore P: 0.6795\n",
      "Episode: 200 Total reward: 10.0 Training loss: 659.9007 Explore P: 0.6788\n",
      "Episode: 201 Total reward: 23.0 Training loss: 912.2581 Explore P: 0.6773\n",
      "Episode: 202 Total reward: 18.0 Training loss: 1757.8516 Explore P: 0.6761\n",
      "Episode: 203 Total reward: 13.0 Training loss: 2358.0161 Explore P: 0.6752\n",
      "Episode: 204 Total reward: 14.0 Training loss: 1715.7592 Explore P: 0.6743\n",
      "Episode: 205 Total reward: 11.0 Training loss: 938.9199 Explore P: 0.6736\n",
      "Episode: 206 Total reward: 14.0 Training loss: 220.3522 Explore P: 0.6726\n",
      "Episode: 207 Total reward: 38.0 Training loss: 1172.8420 Explore P: 0.6701\n",
      "Episode: 208 Total reward: 15.0 Training loss: 1542.0652 Explore P: 0.6691\n",
      "Episode: 209 Total reward: 15.0 Training loss: 1570.5479 Explore P: 0.6681\n",
      "Episode: 210 Total reward: 15.0 Training loss: 106.2957 Explore P: 0.6671\n",
      "Episode: 211 Total reward: 38.0 Training loss: 2152.5308 Explore P: 0.6647\n",
      "Episode: 212 Total reward: 43.0 Training loss: 1039.3124 Explore P: 0.6618\n",
      "Episode: 213 Total reward: 17.0 Training loss: 109.9241 Explore P: 0.6607\n",
      "Episode: 214 Total reward: 14.0 Training loss: 919.7225 Explore P: 0.6598\n",
      "Episode: 215 Total reward: 11.0 Training loss: 1757.9078 Explore P: 0.6591\n",
      "Episode: 216 Total reward: 17.0 Training loss: 3109.4160 Explore P: 0.6580\n",
      "Episode: 217 Total reward: 36.0 Training loss: 885.4401 Explore P: 0.6557\n",
      "Episode: 218 Total reward: 15.0 Training loss: 4533.6807 Explore P: 0.6547\n",
      "Episode: 219 Total reward: 15.0 Training loss: 894.5688 Explore P: 0.6537\n",
      "Episode: 220 Total reward: 16.0 Training loss: 858.3693 Explore P: 0.6527\n",
      "Episode: 221 Total reward: 18.0 Training loss: 2885.5310 Explore P: 0.6516\n",
      "Episode: 222 Total reward: 9.0 Training loss: 1000.7389 Explore P: 0.6510\n",
      "Episode: 223 Total reward: 8.0 Training loss: 3347.4250 Explore P: 0.6505\n",
      "Episode: 224 Total reward: 13.0 Training loss: 1492.6809 Explore P: 0.6496\n",
      "Episode: 225 Total reward: 11.0 Training loss: 897.7858 Explore P: 0.6489\n",
      "Episode: 226 Total reward: 12.0 Training loss: 1075.9736 Explore P: 0.6482\n",
      "Episode: 227 Total reward: 41.0 Training loss: 3766.0637 Explore P: 0.6456\n",
      "Episode: 228 Total reward: 30.0 Training loss: 1574.5160 Explore P: 0.6437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 229 Total reward: 15.0 Training loss: 1052.1327 Explore P: 0.6427\n",
      "Episode: 230 Total reward: 14.0 Training loss: 1217.5291 Explore P: 0.6418\n",
      "Episode: 231 Total reward: 10.0 Training loss: 1698.3070 Explore P: 0.6412\n",
      "Episode: 232 Total reward: 9.0 Training loss: 849.6009 Explore P: 0.6406\n",
      "Episode: 233 Total reward: 24.0 Training loss: 1700.3365 Explore P: 0.6391\n",
      "Episode: 234 Total reward: 21.0 Training loss: 1563.9998 Explore P: 0.6378\n",
      "Episode: 235 Total reward: 24.0 Training loss: 184.1069 Explore P: 0.6363\n",
      "Episode: 236 Total reward: 14.0 Training loss: 1826.1389 Explore P: 0.6354\n",
      "Episode: 237 Total reward: 20.0 Training loss: 794.8887 Explore P: 0.6342\n",
      "Episode: 238 Total reward: 9.0 Training loss: 558.7684 Explore P: 0.6336\n",
      "Episode: 239 Total reward: 20.0 Training loss: 970.0158 Explore P: 0.6324\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "rewards_list = []\n",
    "loss = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    total_steps = 0\n",
    "    for episode in range(1, train_episodes):\n",
    "        total_reward = 0\n",
    "        current_step = 0\n",
    "        while current_step < max_steps:\n",
    "            total_steps += 1\n",
    "            \n",
    "            # Determine whether to explore or exploit\n",
    "            explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*total_steps)\n",
    "            if explore_p > np.random.rand():\n",
    "                # Explore a random action\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Exploit best action\n",
    "                feed = {mainQN.inputs_: state.reshape((1, *state.shape))}\n",
    "                Qs = sess.run(mainQN.output, feed_dict=feed)\n",
    "                action = np.argmax(Qs)\n",
    "                \n",
    "            # Execute action to get new state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                # the episode ends so no next state\n",
    "                next_state = np.zeros(state.shape)\n",
    "                current_step = max_steps\n",
    "                \n",
    "                print('Episode: {}'.format(episode),\n",
    "                      'Total reward: {}'.format(total_reward),\n",
    "                      'Training loss: {:.4f}'.format(loss),\n",
    "                      'Explore P: {:.4f}'.format(explore_p))\n",
    "                rewards_list.append((episode, total_reward))\n",
    "                \n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                \n",
    "                # Start new episode\n",
    "                env.reset()\n",
    "                # Take a random step to get the cart moving\n",
    "                state, reward, done, _ = env.step(env.action_space.sample())\n",
    "                \n",
    "            else:\n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                state - next_state\n",
    "                current_step += 1\n",
    "                \n",
    "            # Everything above is just to fill out the memory and let the agent play the game, this is where we actually train\n",
    "            batch = memory.sample(batch_size)\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            rewards = np.array([each[2] for each in batch])\n",
    "            next_states = np.array([each[3] for each in batch])\n",
    "            \n",
    "            target_Qs = sess.run(mainQN.output, feed_dict={mainQN.inputs_: next_states})\n",
    "            # Set target_Qs to 0 for states where episode ends\n",
    "            episode_ends = (next_states == np.zeros(states[0].shape)).all(axis=1)\n",
    "            target_Qs[episode_ends] = (0, 0)\n",
    "            \n",
    "            targets = rewards + gamma * np.max(target_Qs, axis=1)\n",
    "            \n",
    "            loss, _ = sess.run([mainQN.loss, mainQN.optimizer],\n",
    "                              feed_dict={mainQN.inputs_: states,\n",
    "                                        mainQN.targetQs_: targets,\n",
    "                                        mainQN.actions_: actions})\n",
    "            \n",
    "    saver.save(sess, \"cartpole.ckpt\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
